{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cross_Entropy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNzLLUAyhLqn/SqVkHMGZq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgamel/learn_n_dev/blob/python_machine_learning/Cross_Entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Cross-Entropy for Machine Learning"
      ],
      "metadata": {
        "id": "4IXpDvBwhqjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-entropy is commonly used in machine learning as a loss function.\n",
        "\n",
        "Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.\n",
        "\n",
        "Cross-entropy is also related to and often confused with logistic loss, called log loss. Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably.\n",
        "\n"
      ],
      "metadata": {
        "id": "21PHPP1jhtBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example:\n",
        "\n",
        "* How to calculate cross-entropy from scratch and using standard machine learning libraries.\n",
        "* Cross-entropy can be used as a loss function when optimizing classification models like logistic regression and artificial neural networks.\n",
        "* Cross-entropy is different from KL divergence but can be calculated using KL divergence, and is different from log loss but calculates the same quantity when used as a loss function."
      ],
      "metadata": {
        "id": "hHeD_1jmtlWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Entropy"
      ],
      "metadata": {
        "id": "aVF6XMqet9lj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-entropy is a measure of the difference between two probability distributions for a given random variable or set of events.\n",
        "\n",
        "You might recall that information quantifies the number of bits required to encode and transmit an event. Lower probability events have more information, higher probability events have less information.\n",
        "\n",
        "In information theory, we like to describe the “surprise” of an event. An event is more surprising the less likely it is, meaning it contains more information.\n",
        "\n",
        "* **Low Probability Event** (surprising): More information.\n",
        "* **Higher Probability Event** (unsurprising): Less information.\n",
        "Information h(x) can be calculated for an event x, given the probability of the event P(x) as follows:\n",
        "\n",
        "h(x) = -log(P(x))\n",
        "\n",
        "**Entropy** is the number of bits required to transmit a randomly selected event from a probability distribution. A skewed distribution has a low entropy, whereas a distribution where events have equal probability has a larger entropy.\n",
        "\n",
        "A skewed probability distribution has less “surprise” and in turn a low entropy because likely events dominate. Balanced distribution are more surprising and turn have higher entropy because events are equally likely.\n",
        "\n",
        "* **Skewed Probability Distribution** (unsurprising): Low entropy.\n",
        "* **Balanced Probability Distribution** (surprising): High entropy.\n",
        "\n",
        "Entropy H(x) can be calculated for a random variable with a set of x in X discrete states discrete states and their probability P(x) as follows:\n",
        "\n",
        "H(X) = – sum x in X P(x) * log(P(x))\n",
        "\n",
        "**Cross-entropy** builds upon the idea of entropy from information theory and calculates the number of bits required to represent or transmit an average event from one distribution compared to another distribution.\n",
        "\n",
        "The intuition for this definition comes if we consider a target or underlying probability distribution P and an approximation of the target distribution Q, then the cross-entropy of Q from P is the number of additional bits to represent an event using Q instead of P.\n",
        "\n",
        "The cross-entropy between two probability distributions, such as Q from P, can be stated formally as:\n",
        "\n",
        "H(P, Q)\n",
        "\n",
        "Where H() is the cross-entropy function, P may be the target distribution and Q is the approximation of the target distribution.\n",
        "\n",
        "Cross-entropy can be calculated using the probabilities of the events from P and Q, as follows:\n",
        "\n",
        "H(P, Q) = – sum x in X P(x) * log(Q(x))\n",
        "\n",
        "Where P(x) is the probability of the event x in P, Q(x) is the probability of event x in Q and log is the base-2 logarithm, meaning that the results are in bits. If the base-e or natural logarithm is used instead, the result will have the units called nats.\n",
        "\n",
        "This calculation is for discrete probability distributions, although a similar calculation can be used for continuous probability distributions using the integral across the events instead of the sum.\n",
        "\n",
        "The result will be a positive number measured in bits and will be equal to the entropy of the distribution if the two probability distributions are identical.\n",
        "\n"
      ],
      "metadata": {
        "id": "RQGsH5qrt-q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Entropy Versus KL Divergence"
      ],
      "metadata": {
        "id": "VGpoKjlQvSzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-entropy is not KL Divergence.\n",
        "\n",
        "Cross-entropy is related to divergence measures, such as the Kullback-Leibler, or KL, Divergence that quantifies how much one distribution differs from another.\n",
        "\n",
        "Specifically, the KL divergence measures a very similar quantity to cross-entropy. It measures the average number of extra bits required to represent a message with Q instead of P, not the total number of bits.\n",
        "\n",
        "As such, the KL divergence is often referred to as the “relative entropy.”\n",
        "\n",
        "* **Cross-Entropy:** Average number of total bits to represent an event from Q instead of P.\n",
        "* **Relative Entropy (KL Divergence):** Average number of extra bits to represent an event from Q instead of P.\n",
        "\n",
        "KL divergence can be calculated as the negative sum of probability of each event in P multiples by the log of the probability of the event in Q over the probability of the event in P. Typically, log base-2 so that the result is measured in bits.\n",
        "\n",
        "KL(P || Q) = – sum x in X P(x) * log(Q(x) / P(x))\n",
        "The value within the sum is the divergence for a given event.\n",
        "\n",
        "As such, we can calculate the cross-entropy by adding the entropy of the distribution plus the additional entropy calculated by the KL divergence. This is intuitive, given the definition of both calculations; for example:\n",
        "\n",
        "H(P, Q) = H(P) + KL(P || Q)\n",
        "Where H(P, Q) is the cross-entropy of Q from P, H(P) is the entropy of P and KL(P || Q) is the divergence of Q from P.\n",
        "\n",
        "Entropy can be calculated for a probability distribution as the negative sum of the probability for each event multiplied by the log of the probability for the event, where log is base-2 to ensure the result is in bits.\n",
        "\n",
        "H(P) = – sum x on X p(x) * log(p(x))\n",
        "Like KL divergence, cross-entropy is not symmetrical, meaning that:\n",
        "\n",
        "H(P, Q) != H(Q, P)\n",
        "As we will see later, both cross-entropy and KL divergence calculate the same quantity when they are used as loss functions for optimizing a classification predictive model. It is under this context that you might sometimes see that cross-entropy and KL divergence are the same."
      ],
      "metadata": {
        "id": "6vs11dEKvT7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to Calculate Cross-Entropy"
      ],
      "metadata": {
        "id": "niymGRNYv2YJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Two Discrete Probability Distributions **\n",
        "\n",
        "Consider a random variable with three discrete events as different colors: red, green, and blue.\n",
        "\n",
        "We may have two different probability distributions for this variable; for example:\n",
        "\n",
        "```\n",
        "# define distributions\n",
        "events = ['red', 'green', 'blue']\n",
        "p = [0.10, 0.40, 0.50]\n",
        "q = [0.80, 0.15, 0.05]\n",
        "```\n",
        "\n",
        "We can plot a bar chart of these probabilities to compare them directly as probability histograms.\n",
        "\n",
        "The complete example is listed below."
      ],
      "metadata": {
        "id": "QOb11ArXv4Ff"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "Al6XIhH6F5IN",
        "outputId": "69bca4e0-8209-4aa8-ae2c-dd9559bb0d1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P=1.000 Q=1.000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# plot of distributions\n",
        "from matplotlib import pyplot\n",
        "# define distributions\n",
        "events = ['red', 'green', 'blue']\n",
        "p = [0.10, 0.40, 0.50]\n",
        "q = [0.80, 0.15, 0.05]\n",
        "print('P=%.3f Q=%.3f' % (sum(p), sum(q)))\n",
        "# plot first distribution\n",
        "pyplot.subplot(2,1,1)\n",
        "pyplot.bar(events, p)\n",
        "# plot second distribution\n",
        "pyplot.subplot(2,1,2)\n",
        "pyplot.bar(events, q)\n",
        "# show the plot\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example creates a histogram for each probability distribution, allowing the probabilities for each event to be directly compared.\n",
        "\n",
        "We can see that indeed the distributions are different."
      ],
      "metadata": {
        "id": "aL7NIbTwwPfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate Cross-Entropy Between Distributions**\n",
        "\n",
        "Next, we can develop a function to calculate the cross-entropy between the two distributions.\n",
        "\n",
        "We will use log base-2 to ensure the result has units in bits.\n",
        "\n",
        "```\n",
        "# calculate cross entropy\n",
        "def cross_entropy(p, q):\n",
        "\treturn -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
        "```\n",
        "\n",
        "We can then use this function to calculate the cross-entropy of P from Q, as well as the reverse, Q from P.\n",
        "\n",
        "```\n",
        "...\n",
        "# calculate cross entropy H(P, Q)\n",
        "ce_pq = cross_entropy(p, q)\n",
        "print('H(P, Q): %.3f bits' % ce_pq)\n",
        "# calculate cross entropy H(Q, P)\n",
        "ce_qp = cross_entropy(q, p)\n",
        "print('H(Q, P): %.3f bits' % ce_qp)\n",
        "```\n",
        "\n",
        "Tying this all together, the complete example is listed below."
      ],
      "metadata": {
        "id": "kNgilUdEwQdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of calculating cross entropy\n",
        "from math import log2\n",
        "\n",
        "# calculate cross entropy\n",
        "def cross_entropy(p, q):\n",
        "\treturn -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
        "\n",
        "# define data\n",
        "p = [0.10, 0.40, 0.50]\n",
        "q = [0.80, 0.15, 0.05]\n",
        "# calculate cross entropy H(P, Q)\n",
        "ce_pq = cross_entropy(p, q)\n",
        "print('H(P, Q): %.3f bits' % ce_pq)\n",
        "# calculate cross entropy H(Q, P)\n",
        "ce_qp = cross_entropy(q, p)\n",
        "print('H(Q, P): %.3f bits' % ce_qp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pec_5zgrwxAr",
        "outputId": "9e8b0698-3e61-4e67-9523-4af6277fce86"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "H(P, Q): 3.288 bits\n",
            "H(Q, P): 2.906 bits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example first calculates the cross-entropy of Q from P as just over 3 bits, then P from Q as just under 3 bits.\n",
        "\n",
        "```\n",
        "H(P, Q): 3.288 bits\n",
        "H(Q, P): 2.906 bits\n",
        "```"
      ],
      "metadata": {
        "id": "ZYMjPVPNw4g0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate Cross-Entropy Between a Distribution and Itself**"
      ],
      "metadata": {
        "id": "ywpGKE4Pw-jS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If two probability distributions are the same, then the cross-entropy between them will be the entropy of the distribution.\n",
        "\n",
        "We can demonstrate this by calculating the cross-entropy of P vs P and Q vs Q.\n",
        "\n",
        "The complete example is listed below."
      ],
      "metadata": {
        "id": "C7O7AWb1xDcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of calculating cross entropy for identical distributions\n",
        "from math import log2\n",
        "\n",
        "# calculate cross entropy\n",
        "def cross_entropy(p, q):\n",
        "\treturn -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
        "\n",
        "# define data\n",
        "p = [0.10, 0.40, 0.50]\n",
        "q = [0.80, 0.15, 0.05]\n",
        "# calculate cross entropy H(P, P)\n",
        "ce_pp = cross_entropy(p, p)\n",
        "print('H(P, P): %.3f bits' % ce_pp)\n",
        "# calculate cross entropy H(Q, Q)\n",
        "ce_qq = cross_entropy(q, q)\n",
        "print('H(Q, Q): %.3f bits' % ce_qq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P39tHcNRxHZx",
        "outputId": "41fa5258-dd73-4f47-ab87-69d060f7e146"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "H(P, P): 1.361 bits\n",
            "H(Q, Q): 0.884 bits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example first calculates the cross-entropy of Q vs Q which is calculated as the entropy for Q, and P vs P which is calculated as the entropy for P.\n",
        "\n",
        "```\n",
        "H(P, P): 1.361 bits\n",
        "H(Q, Q): 0.884 bits\n",
        "```"
      ],
      "metadata": {
        "id": "c1z9cTBExS2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate Cross-Entropy Using KL Divergence**\n",
        "\n",
        "We can also calculate the cross-entropy using the KL divergence.\n",
        "\n",
        "The cross-entropy calculated with KL divergence should be identical, and it may be interesting to calculate the KL divergence between the distributions as well to see the relative entropy or additional bits required instead of the total bits calculated by the cross-entropy.\n",
        "\n",
        "First, we can define a function to calculate the KL divergence between the distributions using log base-2 to ensure the result is also in bits.\n",
        "\n",
        "```\n",
        "# calculate the kl divergence KL(P || Q)\n",
        "def kl_divergence(p, q):\n",
        "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
        "```\n",
        "\n",
        "Next, we can define a function to calculate the entropy for a given probability distribution.\n",
        "\n",
        "```\n",
        "# calculate entropy H(P)\n",
        "def entropy(p):\n",
        "\treturn -sum([p[i] * log2(p[i]) for i in range(len(p))])\n",
        "Finally, we can calculate the cross-entropy using the entropy() and kl_divergence() functions.\n",
        "\n",
        "# calculate cross entropy H(P, Q)\n",
        "def cross_entropy(p, q):\n",
        "\treturn entropy(p) + kl_divergence(p, q)\n",
        "To keep the example simple, we can compare the cross-entropy for H(P, Q) to the KL divergence KL(P || Q) and the entropy H(P).\n",
        "```\n",
        "\n",
        "The complete example is listed below."
      ],
      "metadata": {
        "id": "BLI8Y2WjxXn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of calculating cross entropy with kl divergence\n",
        "from math import log2\n",
        "\n",
        "# calculate the kl divergence KL(P || Q)\n",
        "def kl_divergence(p, q):\n",
        "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
        "\n",
        "# calculate entropy H(P)\n",
        "def entropy(p):\n",
        "\treturn -sum([p[i] * log2(p[i]) for i in range(len(p))])\n",
        "\n",
        "# calculate cross entropy H(P, Q)\n",
        "def cross_entropy(p, q):\n",
        "\treturn entropy(p) + kl_divergence(p, q)\n",
        "\n",
        "# define data\n",
        "p = [0.10, 0.40, 0.50]\n",
        "q = [0.80, 0.15, 0.05]\n",
        "# calculate H(P)\n",
        "en_p = entropy(p)\n",
        "print('H(P): %.3f bits' % en_p)\n",
        "# calculate kl divergence KL(P || Q)\n",
        "kl_pq = kl_divergence(p, q)\n",
        "print('KL(P || Q): %.3f bits' % kl_pq)\n",
        "# calculate cross entropy H(P, Q)\n",
        "ce_pq = cross_entropy(p, q)\n",
        "print('H(P, Q): %.3f bits' % ce_pq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcZgTR_vxz0F",
        "outputId": "ccb18167-7078-4537-bf89-3a045029f5ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "H(P): 1.361 bits\n",
            "KL(P || Q): 1.927 bits\n",
            "H(P, Q): 3.288 bits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example, we can see that the cross-entropy score of 3.288 bits is comprised of the entropy of P 1.361 and the additional 1.927 bits calculated by the KL divergence.\n",
        "\n",
        "This is a useful example that clearly illustrates the relationship between all three calculations.\n",
        "\n",
        "```\n",
        "H(P): 1.361 bits\n",
        "KL(P || Q): 1.927 bits\n",
        "H(P, Q): 3.288 bits\n",
        "```"
      ],
      "metadata": {
        "id": "2s7INVh6x9dY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Entropy as a Loss Function"
      ],
      "metadata": {
        "id": "Qz_qwuc_yAKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-entropy is widely used as a loss function when optimizing classification models.\n",
        "\n",
        "Two examples that you may encounter include the logistic regression algorithm (a linear classification algorithm), and artificial neural networks that can be used for classification tasks.\n",
        "\n",
        "Classification problems are those that involve one or more input variables and the prediction of a class label.\n",
        "\n",
        "Classification tasks that have just two labels for the output variable are referred to as binary classification problems, whereas those problems with more than two labels are referred to as categorical or multi-class classification problems.\n",
        "\n",
        "* **Binary Classification:** Task of predicting one of two class labels for a given example.\n",
        "* **Multi-Class Classification:** Task of predicting one of more than two class labels for a given example.\n",
        "\n",
        "We can see that the idea of cross-entropy may be useful for optimizing a classification model.\n",
        "\n",
        "Each example has a known class label with a probability of 1.0, and a probability of 0.0 for all other labels. A model can estimate the probability of an example belonging to each class label. Cross-entropy can then be used to calculate the difference between the two probability distributions.\n",
        "\n",
        "As such, we can map the classification of one example onto the idea of a random variable with a probability distribution as follows:\n",
        "\n",
        "* **Random Variable:** The example for which we require a predicted class label.\n",
        "* **Events:** Each class label that could be predicted.\n",
        "\n",
        "In classification tasks, we know the target probability distribution P for an input as the class label 0 or 1 interpreted as probabilities as “impossible” or “certain” respectively. These probabilities have no surprise at all, therefore they have no information content or zero entropy.\n",
        "\n",
        "Our model seeks to approximate the target probability distribution Q.\n",
        "\n",
        "In the language of classification, these are the actual and the predicted probabilities, or y and yhat.\n",
        "\n",
        "* **Expected Probability (y):** The known probability of each class label for an example in the dataset (P).\n",
        "* **Predicted Probability (yhat):** The probability of each class label an example predicted by the model (Q).\n",
        "\n",
        "We can, therefore, estimate the cross-entropy for a single prediction using the cross-entropy calculation described above; for example.\n",
        "\n",
        "H(P, Q) = – sum x in X P(x) * log(Q(x))\n",
        "\n",
        "Where each x in X is a class label that could be assigned to the example, and P(x) will be 1 for the known label and 0 for all other labels.\n",
        "\n",
        "The cross-entropy for a single example in a binary classification task can be stated by unrolling the sum operation as follows:\n",
        "\n",
        "H(P, Q) = – (P(class0) * log(Q(class0)) + P(class1) * log(Q(class1)))\n",
        "\n",
        "You may see this form of calculating cross-entropy cited in textbooks.\n",
        "\n",
        "If there are just two class labels, the probability is modeled as the Bernoulli distribution for the positive class label. This means that the probability for class 1 is predicted by the model directly, and the probability for class 0 is given as one minus the predicted probability, for example:\n",
        "\n",
        "* **Predicted P(class0)** = 1 – yhat\n",
        "* **Predicted P(class1)** = yhat\n",
        "\n",
        "When calculating cross-entropy for classification tasks, the base-e or natural logarithm is used. This means that the units are in nats, not bits.\n",
        "\n",
        "We are often interested in minimizing the cross-entropy for the model across the entire training dataset. This is calculated by calculating the average cross-entropy across all training examples."
      ],
      "metadata": {
        "id": "_9_RRBpdyH_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Entropy for Class Labels"
      ],
      "metadata": {
        "id": "ffgXaQZMzSNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that when two distributions are identical, the cross-entropy between them is equal to the entropy for the probability distribution.\n",
        "\n",
        "Class labels are encoded using the values 0 and 1 when preparing data for classification tasks.\n",
        "\n",
        "For example, if a classification problem has three classes, and an example has a label for the first class, then the probability distribution will be [1, 0, 0]. If an example has a label for the second class, it will have a probability distribution for the two events as [0, 1, 0]. This is called a one hot encoding.\n",
        "\n",
        "This probability distribution has no information as the outcome is certain. We know the class. Therefore the entropy for this variable is zero.\n",
        "\n",
        "This is an important concept and we can demonstrate it with a worked example.\n",
        "\n",
        "Pretend with have a classification problem with 3 classes, and we have one example that belongs to each class. We can represent each example as a discrete probability distribution with a 1.0 probability for the class to which the example belongs and a 0.0 probability for all other classes.\n",
        "\n",
        "We can calculate the entropy of the probability distribution for each “variable” across the “events“.\n",
        "\n",
        "The complete example is listed below.\n",
        "\n"
      ],
      "metadata": {
        "id": "Gui6zURHy4Fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# entropy of examples from a classification task with 3 classes\n",
        "from math import log2\n",
        "from numpy import asarray\n",
        "\n",
        "# calculate entropy\n",
        "def entropy(p):\n",
        "\treturn -sum([p[i] * log2(p[i]) for i in range(len(p))])\n",
        "\n",
        "# class 1\n",
        "p = asarray([1,0,0]) + 1e-15\n",
        "print(entropy(p))\n",
        "# class 2\n",
        "p = asarray([0,1,0]) + 1e-15\n",
        "print(entropy(p))\n",
        "# class 3\n",
        "p = asarray([0,0,1]) + 1e-15\n",
        "print(entropy(p))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1oJSZEpzo8_",
        "outputId": "c53c9a96-8623-47d3-c5de-bcdf2b8965f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.805612959471341e-14\n",
            "9.805612959471341e-14\n",
            "9.805612959471341e-14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example calculates the entropy for each random variable.\n",
        "\n",
        "We can see that in each case, the entropy is 0.0 (actually a number very close to zero).\n",
        "\n",
        "Note that we had to add a very small value to the 0.0 values to avoid the log() from blowing up, as we cannot calculate the log of 0.0.\n",
        "\n",
        "As such, the entropy of a known class label is always 0.0.\n",
        "\n",
        "This means that the cross entropy of two distributions (real and predicted) that have the same probability distribution for a class label, will also always be 0.0.\n",
        "\n",
        "Recall that when evaluating a model using cross-entropy on a training dataset that we average the cross-entropy across all examples in the dataset.\n",
        "\n",
        "Therefore, a cross-entropy of 0.0 when training a model indicates that the predicted class probabilities are identical to the probabilities in the training dataset, e.g. zero loss.\n",
        "\n",
        "We could just as easily minimize the KL divergence as a loss function instead of the cross-entropy.\n",
        "\n",
        "Recall that the KL divergence is the extra bits required to transmit one variable compared to another. It is the cross-entropy without the entropy of the class label, which we know would be zero anyway.\n",
        "\n",
        "As such, minimizing the KL divergence and the cross entropy for a classification task are identical.\n",
        "\n",
        "In practice, a cross-entropy loss of 0.0 often indicates that the model has overfit the training dataset, but that is another story.\n"
      ],
      "metadata": {
        "id": "94LTjAPMzt_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Cross-Entropy Between Class Labels and Probabilities"
      ],
      "metadata": {
        "id": "uZ5muANt0EgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The use of cross-entropy for classification often gives different specific names based on the number of classes, mirroring the name of the classification task; for example:\n",
        "\n",
        "* **Binary Cross-Entropy:** Cross-entropy as a loss function for a binary classification task.\n",
        "* **Categorical Cross-Entropy:** Cross-entropy as a loss function for a multi-class classification task.\n",
        "\n",
        "We can make the use of cross-entropy as a loss function concrete with a worked example.\n",
        "\n",
        "Consider a two-class classification task with the following 10 actual class labels (P) and predicted class labels (Q).\n",
        "\n",
        "```\n",
        "...\n",
        "# define classification data\n",
        "p = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
        "q = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3]\n",
        "```\n",
        "\n",
        "We can enumerate these probabilities and calculate the cross-entropy for each using the cross-entropy function developed in the previous section using log() (natural logarithm) instead of log2().\n",
        "\n",
        "```\n",
        "# calculate cross entropy\n",
        "def cross_entropy(p, q):\n",
        "\treturn -sum([p[i]*log(q[i]) for i in range(len(p))])\n",
        "```\n",
        "\n",
        "For each actual and predicted probability, we must convert the prediction into a distribution of probabilities across each event, in this case, the classes {0, 1} as 1 minus the probability for class 0 and probability for class 1.\n",
        "\n",
        "We can then calculate the cross-entropy and repeat the process for all examples.\n",
        "\n",
        "```\n",
        "...\n",
        "# calculate cross entropy for each example\n",
        "results = list()\n",
        "for i in range(len(p)):\n",
        "\t# create the distribution for each event {0, 1}\n",
        "\texpected = [1.0 - p[i], p[i]]\n",
        "\tpredicted = [1.0 - q[i], q[i]]\n",
        "\t# calculate cross entropy for the two events\n",
        "\tce = cross_entropy(expected, predicted)\n",
        "\tprint('>[y=%.1f, yhat=%.1f] ce: %.3f nats' % (p[i], q[i], ce))\n",
        "\tresults.append(ce)\n",
        "```\n",
        "\n",
        "Finally, we can calculate the average cross-entropy across the dataset and report it as the cross-entropy loss for the model on the dataset.\n",
        "\n",
        "```\n",
        "...\n",
        "# calculate the average cross entropy\n",
        "mean_ce = mean(results)\n",
        "print('Average Cross Entropy: %.3f nats' % mean_ce)\n",
        "```\n",
        "\n",
        "Tying this all together, the complete example is listed below."
      ],
      "metadata": {
        "id": "cNQUCF5k0II3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate cross entropy for classification problem\n",
        "from math import log\n",
        "from numpy import mean\n",
        "\n",
        "# calculate cross entropy\n",
        "def cross_entropy(p, q):\n",
        "\treturn -sum([p[i]*log(q[i]) for i in range(len(p))])\n",
        "\n",
        "# define classification data\n",
        "p = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
        "q = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3]\n",
        "# calculate cross entropy for each example\n",
        "results = list()\n",
        "for i in range(len(p)):\n",
        "\t# create the distribution for each event {0, 1}\n",
        "\texpected = [1.0 - p[i], p[i]]\n",
        "\tpredicted = [1.0 - q[i], q[i]]\n",
        "\t# calculate cross entropy for the two events\n",
        "\tce = cross_entropy(expected, predicted)\n",
        "\tprint('>[y=%.1f, yhat=%.1f] ce: %.3f nats' % (p[i], q[i], ce))\n",
        "\tresults.append(ce)\n",
        "\n",
        "# calculate the average cross entropy\n",
        "mean_ce = mean(results)\n",
        "print('Average Cross Entropy: %.3f nats' % mean_ce)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hT_o5Q_04SO",
        "outputId": "6bc3bbef-8e1c-44df-c8b6-b2d5ab3a45f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">[y=1.0, yhat=0.8] ce: 0.223 nats\n",
            ">[y=1.0, yhat=0.9] ce: 0.105 nats\n",
            ">[y=1.0, yhat=0.9] ce: 0.105 nats\n",
            ">[y=1.0, yhat=0.6] ce: 0.511 nats\n",
            ">[y=1.0, yhat=0.8] ce: 0.223 nats\n",
            ">[y=0.0, yhat=0.1] ce: 0.105 nats\n",
            ">[y=0.0, yhat=0.4] ce: 0.511 nats\n",
            ">[y=0.0, yhat=0.2] ce: 0.223 nats\n",
            ">[y=0.0, yhat=0.1] ce: 0.105 nats\n",
            ">[y=0.0, yhat=0.3] ce: 0.357 nats\n",
            "Average Cross Entropy: 0.247 nats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example prints the actual and predicted probabilities for each example and the cross-entropy in nats.\n",
        "\n",
        "The final average cross-entropy loss across all examples is reported, in this case, as 0.247 nats.\n",
        "\n",
        "This is how cross-entropy loss is calculated when optimizing a logistic regression model or a neural network model under a cross-entropy loss function."
      ],
      "metadata": {
        "id": "JaWGWQX51DEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Cross-Entropy Using Keras"
      ],
      "metadata": {
        "id": "GGkmerD41LR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can confirm the same calculation by using the binary_crossentropy() function from the Keras deep learning API to calculate the cross-entropy loss for our small dataset.\n",
        "\n",
        "The complete example is listed below.\n",
        "\n",
        "Note: This example assumes that you have the Keras library installed (e.g. version 2.3 or higher) and configured with a backend library such as TensorFlow (version 2.0 or higher). If not, you can skip running this example."
      ],
      "metadata": {
        "id": "bJKjcPvP1MgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate cross entropy with keras\n",
        "from numpy import asarray\n",
        "from keras import backend\n",
        "from keras.losses import binary_crossentropy\n",
        "# prepare classification data\n",
        "p = asarray([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
        "q = asarray([0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3])\n",
        "# convert to keras variables\n",
        "y_true = backend.variable(p)\n",
        "y_pred = backend.variable(q)\n",
        "# calculate the average cross-entropy\n",
        "mean_ce = backend.eval(binary_crossentropy(y_true, y_pred))\n",
        "print('Average Cross Entropy: %.3f nats' % mean_ce)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDJk5q6y1WHH",
        "outputId": "c9aa50f9-8ac2-4985-e489-74a9c9f0c3af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Cross Entropy: 0.247 nats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example, we can see that the same average cross-entropy loss of 0.247 nats is reported.\n",
        "\n",
        "This confirms the correct manual calculation of cross-entropy."
      ],
      "metadata": {
        "id": "9L2XjH3j1zaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intuition for Cross-Entropy on Predicted Probabilities"
      ],
      "metadata": {
        "id": "s7UngPNi14PJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can further develop the intuition for the cross-entropy for predicted class probabilities.\n",
        "\n",
        "For example, given that an average cross-entropy loss of 0.0 is a perfect model, what do average cross-entropy values greater than zero mean exactly?\n",
        "\n",
        "We can explore this question no a binary classification problem where the class labels as 0 and 1. This is a discrete probability distribution with two events and a certain probability for one event and an impossible probability for the other event.\n",
        "\n",
        "We can then calculate the cross entropy for different “predicted” probability distributions transitioning from a perfect match of the target distribution to the exact opposite probability distribution.\n",
        "\n",
        "We would expect that as the predicted probability distribution diverges further from the target distribution that the cross-entropy calculated will increase.\n",
        "\n",
        "The example below implements this and plots the cross-entropy result for the predicted probability distribution compared to the target of [0, 1] for two events as we would see for the cross-entropy in a binary classification task."
      ],
      "metadata": {
        "id": "4gnWtQ4A19D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross-entropy for predicted probability distribution vs label\n",
        "from math import log\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# calculate cross-entropy\n",
        "def cross_entropy(p, q, ets=1e-15):\n",
        "\treturn -sum([p[i]*log(q[i]+ets) for i in range(len(p))])\n",
        "\n",
        "# define the target distribution for two events\n",
        "target = [0.0, 1.0]\n",
        "# define probabilities for the first event\n",
        "probs = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]\n",
        "# create probability distributions for the two events\n",
        "dists = [[1.0 - p, p] for p in probs]\n",
        "# calculate cross-entropy for each distribution\n",
        "ents = [cross_entropy(target, d) for d in dists]\n",
        "# plot probability distribution vs cross-entropy\n",
        "pyplot.plot([1-p for p in probs], ents, marker='.')\n",
        "pyplot.title('Probability Distribution vs Cross-Entropy')\n",
        "pyplot.xticks([1-p for p in probs], ['[%.1f,%.1f]'%(d[0],d[1]) for d in dists], rotation=70)\n",
        "pyplot.subplots_adjust(bottom=0.2)\n",
        "pyplot.xlabel('Probability Distribution')\n",
        "pyplot.ylabel('Cross-Entropy (nats)')\n",
        "pyplot.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "t3vBNld92Hx4",
        "outputId": "05d0a811-9ca6-4389-c808-d822ccb263a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example calculates the cross-entropy score for each probability distribution then plots the results as a line plot.\n",
        "\n",
        "We can see that as expected, cross-entropy starts at 0.0 (far left point) when the predicted probability distribution matches the target distribution, then steadily increases as the predicted probability distribution diverges.\n",
        "\n",
        "We can also see a dramatic leap in cross-entropy when the predicted probability distribution is the exact opposite of the target distribution, that is, [1, 0] compared to the target of [0, 1]."
      ],
      "metadata": {
        "id": "o9CyUeNF2P2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are not going to have a model that predicts the exact opposite probability distribution for all cases on a binary classification task.\n",
        "\n",
        "As such, we can remove this case and re-calculate the plot.\n",
        "\n",
        "The updated version of the code is listed below."
      ],
      "metadata": {
        "id": "CfeKLWCq2XRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross-entropy for predicted probability distribution vs label\n",
        "from math import log\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# calculate cross-entropy\n",
        "def cross_entropy(p, q, ets=1e-15):\n",
        "\treturn -sum([p[i]*log(q[i]+ets) for i in range(len(p))])\n",
        "\n",
        "# define the target distribution for two events\n",
        "target = [0.0, 1.0]\n",
        "# define probabilities for the first event\n",
        "probs = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
        "# create probability distributions for the two events\n",
        "dists = [[1.0 - p, p] for p in probs]\n",
        "# calculate cross-entropy for each distribution\n",
        "ents = [cross_entropy(target, d) for d in dists]\n",
        "# plot probability distribution vs cross-entropy\n",
        "pyplot.plot([1-p for p in probs], ents, marker='.')\n",
        "pyplot.title('Probability Distribution vs Cross-Entropy')\n",
        "pyplot.xticks([1-p for p in probs], ['[%.1f,%.1f]'%(d[0],d[1]) for d in dists], rotation=70)\n",
        "pyplot.subplots_adjust(bottom=0.2)\n",
        "pyplot.xlabel('Probability Distribution')\n",
        "pyplot.ylabel('Cross-Entropy (nats)')\n",
        "pyplot.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "5X0G0h0F2c2O",
        "outputId": "0352a22f-68f9-4e38-8f9b-ac34d7969fe4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example gives a much better idea of the relationship between the divergence in probability distribution and the calculated cross-entropy.\n",
        "\n",
        "We can see a super-linear relationship where the more the predicted probability distribution diverges from the target, the larger the increase in cross-entropy.\n",
        "\n",
        "A plot like this can be used as a guide for interpreting the average cross-entropy reported for a model for a binary classification dataset.\n",
        "\n",
        "For example, you can use these cross-entropy values to interpret the mean cross-entropy reported by Keras for a neural network model on a binary classification task, or a binary classification model in scikit-learn evaluated using the logloss metric.\n",
        "\n",
        "You can use it to answer the general question:\n",
        "\n",
        "What is a good cross-entropy score?\n",
        "\n",
        "If you are working in nats (and you usually are) and you are getting mean cross-entropy less than 0.2, you are off to a good start, and less than 0.1 or 0.05 is even better.\n",
        "\n",
        "On the other hand, if you are getting mean cross-entropy greater than 0.2 or 0.3 you can probably improve, and if you are getting a mean cross-entropy greater than 1.0, then something is going on and you’re making poor probability predictions on many examples in your dataset.\n",
        "\n",
        "We can summarise these intuitions for the mean cross-entropy as follows:\n",
        "\n",
        "* Cross-Entropy = 0.00: Perfect probabilities.\n",
        "* Cross-Entropy < 0.02: Great probabilities.\n",
        "* Cross-Entropy < 0.05: On the right track.\n",
        "* Cross-Entropy < 0.20: Fine.\n",
        "* Cross-Entropy > 0.30: Not great.\n",
        "* Cross-Entropy > 1.00: Terrible.\n",
        "* Cross-Entropy > 2.00 Something is broken.\n",
        "\n",
        "This listing will provide a useful guide when interpreting a cross-entropy (log loss) from your logistic regression model, or your artificial neural network model.\n",
        "\n",
        "You can also calculate separate mean cross-entropy scores per-class and help tease out on which classes you’re model has good probabilities, and which it might be messing up."
      ],
      "metadata": {
        "id": "ausvW-EY2lEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Entropy Versus Log Loss"
      ],
      "metadata": {
        "id": "HjOJjZ6f3BtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-Entropy is not Log Loss, but they calculate the same quantity when used as loss functions for classification problems.\n",
        "\n",
        "**Log Loss is the Negative Log Likelihood**\n",
        "\n",
        "Logistic loss refers to the loss function commonly used to optimize a logistic regression model.\n",
        "\n",
        "It may also be referred to as logarithmic loss (which is confusing) or simply log loss.\n",
        "\n",
        "Many models are optimized under a probabilistic framework called the maximum likelihood estimation, or MLE, that involves finding a set of parameters that best explain the observed data.\n",
        "\n",
        "This involves selecting a likelihood function that defines how likely a set of observations (data) are given model parameters. When a log likelihood function is used (which is common), it is often referred to as optimizing the log likelihood for the model. Because it is more common to minimize a function than to maximize it in practice, the log likelihood function is inverted by adding a negative sign to the front. This transforms it into a Negative Log Likelihood function or NLL for short.\n",
        "\n",
        "In deriving the log likelihood function under a framework of maximum likelihood estimation for Bernoulli probability distribution functions (two classes), the calculation comes out to be:\n",
        "\n",
        "* negative log-likelihood(P, Q) = -(P(class0) * log(Q(class0)) + P(class1) * log(Q(class1)))\n",
        "\n",
        "This quantity can be averaged over all training examples by calculating the average of the log of the likelihood function.\n",
        "\n",
        "Negative log-likelihood for binary classification problems is often shortened to simply “log loss” as the loss function derived for logistic regression.\n",
        "\n",
        "* log loss = negative log-likelihood, under a Bernoulli probability distribution\n",
        "\n",
        "We can see that the negative log-likelihood is the same calculation as is used for the cross-entropy for Bernoulli probability distribution functions (two events or classes). In fact, the negative log-likelihood for Multinoulli distributions (multi-class classification) also matches the calculation for cross-entropy."
      ],
      "metadata": {
        "id": "Z57cWMNM3Enx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log Loss and Cross Entropy Calculate the Same Thing"
      ],
      "metadata": {
        "id": "ih5NAIji3y2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For classification problems, “log loss“, “cross-entropy” and “negative log-likelihood” are used interchangeably.\n",
        "\n",
        "More generally, the terms “cross-entropy” and “negative log-likelihood” are used interchangeably in the context of loss functions for classification models.\n",
        "\n",
        "Therefore, calculating log loss will give the same quantity as calculating the cross-entropy for Bernoulli probability distribution. We can confirm this by calculating the log loss using the log_loss() function from the scikit-learn API.\n",
        "\n",
        "Calculating the average log loss on the same set of actual and predicted probabilities from the previous section should give the same result as calculating the average cross-entropy.\n",
        "\n",
        "The complete example is listed below."
      ],
      "metadata": {
        "id": "_SGupi9N3z5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate log loss for classification problem with scikit-learn\n",
        "from sklearn.metrics import log_loss\n",
        "from numpy import asarray\n",
        "# define classification data\n",
        "p = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
        "q = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3]\n",
        "# define data as expected, e.g. probability for each event {0, 1}\n",
        "y_true = asarray([[1-v, v] for v in p])\n",
        "y_pred = asarray([[1-v, v] for v in q])\n",
        "# calculate the average log loss\n",
        "ll = log_loss(y_true, y_pred)\n",
        "print('Average Log Loss: %.3f' % ll)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnNx_QSf37da",
        "outputId": "c9014d0f-9907-4596-c165-e3109f6591ec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Log Loss: 0.247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example gives the expected result of 0.247 log loss, which matches 0.247 nats when calculated using the average cross-entropy."
      ],
      "metadata": {
        "id": "4ut-GL_p4BCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This does not mean that log loss calculates cross-entropy or cross-entropy calculates log loss.\n",
        "\n",
        "Instead, they are different quantities, arrived at from different fields of study, that under the conditions of calculating a loss function for a classification task, result in an equivalent calculation and result. Specifically, a cross-entropy loss function is equivalent to a maximum likelihood function under a Bernoulli or Multinoulli probability distribution.\n",
        "\n",
        "This demonstrates a connection between the study of maximum likelihood estimation and information theory for discrete probability distributions.\n",
        "\n",
        "It is not limited to discrete probability distributions, and this fact is surprising to many practitioners that hear it for the first time.\n",
        "\n",
        "Specifically, a linear regression optimized under the maximum likelihood estimation framework assumes a Gaussian continuous probability distribution for the target variable and involves minimizing the mean squared error function. This is equivalent to the cross-entropy for a random variable with a Gaussian probability distribution.\n",
        "\n",
        "This is a little mind blowing, and comes from the field of differential entropy for continuous random variables.\n",
        "\n",
        "It means that if you calculate the mean squared error between two Gaussian random variables that cover the same events (have the same mean and standard deviation), then you are calculating the cross-entropy between the variables.\n",
        "\n",
        "It also means that if you are using mean squared error loss to optimize your neural network model for a regression problem, you are in effect using a cross entropy loss."
      ],
      "metadata": {
        "id": "ciSyDJU64K1j"
      }
    }
  ]
}