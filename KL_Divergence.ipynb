{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KL_Divergence.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMAR5oAvx4fqWbgAS9HgKhY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgamel/learn_n_dev/blob/python_modeling_forecasting/KL_Divergence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate the KL Divergence for Machine Learning"
      ],
      "metadata": {
        "id": "FDMcq8rcemze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is often desirable to quantify the difference between probability distributions for a given random variable.\n",
        "\n",
        "This occurs frequently in machine learning, when we may be interested in calculating the difference between an actual and observed probability distribution.\n",
        "\n",
        "This can be achieved using techniques from information theory, such as the Kullback-Leibler Divergence (KL divergence), or relative entropy, and the Jensen-Shannon Divergence that provides a normalized and symmetrical version of the KL divergence. These scoring methods can be used as shortcuts in the calculation of other widely used methods, such as mutual information for feature selection prior to modeling, and cross-entropy used as a loss function for many different classifier models.\n",
        "\n",
        "In this post, you will discover how to calculate the divergence between probability distributions."
      ],
      "metadata": {
        "id": "YH9BwWQIevT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Statistical distance is the general idea of calculating the difference between statistical objects like different probability distributions for a random variable.\n",
        "* Kullback-Leibler divergence calculates a score that measures the divergence of one probability distribution from another.\n",
        "* Jensen-Shannon divergence extends KL divergence to calculate a symmetrical score and distance measure of one probability distribution from another."
      ],
      "metadata": {
        "id": "6_fBXgvUyaQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical Distance"
      ],
      "metadata": {
        "id": "sxjPHLz3zEto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many situations where we may want to compare two probability distributions.\n",
        "\n",
        "Specifically, we may have a single random variable and two different probability distributions for the variable, such as a true distribution and an approximation of that distribution.\n",
        "\n",
        "In situations like this, it can be useful to quantify the difference between the distributions. Generally, this is referred to as the problem of calculating the statistical distance between two [statistical objects](https://en.wikipedia.org/wiki/Statistical_distance), e.g. probability distributions.\n",
        "\n",
        "One approach is to calculate a distance measure between the two distributions. This can be challenging as it can be difficult to interpret the measure.\n",
        "\n",
        "Instead, it is more common to calculate a [divergence](https://en.wikipedia.org/wiki/Divergence_(statistics)) between two probability distributions. A divergence is like a measure but is not symmetrical. This means that a divergence is a scoring of how one distribution differs from another, where calculating the divergence for distributions P and Q would give a different score from Q and P.\n",
        "\n",
        "Divergence scores are an important foundation for many different calculations in information theory and more generally in machine learning. For example, they provide shortcuts for calculating scores such as mutual information (information gain) and cross-entropy used as a loss function for classification models.\n",
        "\n",
        "Divergence scores are also used directly as tools for understanding complex modeling problems, such as approximating a target probability distribution when optimizing generative adversarial network (GAN) models.\n",
        "\n",
        "Two commonly used divergence scores from information theory are Kullback-Leibler Divergence and Jensen-Shannon Divergence.\n",
        "\n",
        "We will take a closer look at both of these scores in the following section."
      ],
      "metadata": {
        "id": "HMwxA00wzY4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kullback-Leibler Divergence"
      ],
      "metadata": {
        "id": "d52O_79e2Sr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [Kullback-Leibler Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) score, or KL divergence score, quantifies how much one probability distribution differs from another probability distribution.\n",
        "\n",
        "The KL divergence between two distributions Q and P is often stated using the following notation:\n",
        "\n",
        "```KL(P || Q)```\n",
        "\n",
        "Where the “||” operator indicates “divergence” or Ps divergence from Q.\n",
        "\n",
        "KL divergence can be calculated as the negative sum of probability of each event in P multiplied by the log of the probability of the event in Q over the probability of the event in P.\n",
        "\n",
        "```KL(P || Q) = – sum x in X P(x) * log(Q(x) / P(x))```\n",
        "\n",
        "The value within the sum is the divergence for a given event.\n",
        "\n",
        "This is the same as the positive sum of probability of each event in P multiplied by the log of the probability of the event in P over the probability of the event in Q (e.g. the terms in the fraction are flipped). This is the more common implementation used in practice.\n",
        "\n",
        "```KL(P || Q) = sum x in X P(x) * log(P(x) / Q(x))```\n",
        "\n",
        "The intuition for the KL divergence score is that when the probability for an event from P is large, but the probability for the same event in Q is small, there is a large divergence. When the probability from P is small and the probability from Q is large, there is also a large divergence, but not as large as the first case.\n",
        "\n",
        "It can be used to measure the divergence between discrete and continuous probability distributions, where in the latter case the integral of the events is calculated instead of the sum of the probabilities of the discrete events.\n",
        "\n",
        "The log can be base-2 to give units in “bits,” or the natural logarithm base-e with units in “nats.” When the score is 0, it suggests that both distributions are identical, otherwise the score is positive.\n",
        "\n",
        "Importantly, the KL divergence score is not symmetrical, for example:\n",
        "\n",
        "```KL(P || Q) != KL(Q || P)```\n",
        "\n",
        "It is named for the two authors of the method Solomon Kullback and Richard Leibler, and is sometimes referred to as “relative entropy.”\n",
        "\n",
        "If we are attempting to approximate an unknown probability distribution, then the target probability distribution from data is P and Q is our approximation of the distribution.\n",
        "\n",
        "In this case, the KL divergence summarizes the number of additional bits (i.e. calculated with the base-2 logarithm) required to represent an event from the random variable. The better our approximation, the less additional information is required.\n",
        "\n",
        "We can make the KL divergence concrete with a worked example.\n",
        "\n",
        "Consider a random variable with three events as different colors. We may have two different probability distributions for this variable; for example:\n",
        "\n",
        "```\n",
        "...\n",
        "# define distributions\n",
        "events = ['red', 'green', 'blue']\n",
        "p = [0.10, 0.40, 0.50]\n",
        "q = [0.80, 0.15, 0.05]\n",
        "```\n",
        "\n",
        "We can plot a bar chart of these probabilities to compare them directly as probability histograms.\n",
        "\n",
        "The complete example is listed below.\n"
      ],
      "metadata": {
        "id": "BS_p101S2ToB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot of distributions\n",
        "from matplotlib import pyplot\n",
        "# define distributions\n",
        "events = ['red', 'green', 'blue']\n",
        "p = [0.10, 0.40, 0.50]\n",
        "q = [0.80, 0.15, 0.05]\n",
        "print('P=%.3f Q=%.3f' % (sum(p), sum(q)))\n",
        "# plot first distribution\n",
        "pyplot.subplot(2,1,1)\n",
        "pyplot.bar(events, p)\n",
        "# plot second distribution\n",
        "pyplot.subplot(2,1,2)\n",
        "pyplot.bar(events, q)\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "bTTFUlL29vxK",
        "outputId": "d8b6aafb-95a1-42f4-a606-c81f95263ad0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P=1.000 Q=1.000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAS1UlEQVR4nO3df6yed13/8eeLbt0028y+tpqlP3aqNtsKyvhy7MAZNTqwBFkxzNCpCYaZhq+M4dcv36SEMHHGuKlxajLMmq8N8xcd1KgHVyzEOXAbw57C2GiXQi3j2zZEC4OZMbel8PaP+yrcuzlnvXvunvv0fM7zkTTnuj6fz3Vf73Ou7HVf+9zXdd2pKiRJ7XrRQhcgSZpfBr0kNc6gl6TGGfSS1DiDXpIad85CFzBoxYoVNTExsdBlSNKism/fvi9X1cqZ+s66oJ+YmGB6enqhy5CkRSXJF2frc+pGkhpn0EtS4wx6SWrcWTdHL+nsNrHtnoUuoVmP3/raeXldz+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklq3EhBn2RTkoNJDiXZ9gLj3pCkkkyOsj9J0umbc9AnWQbcAbwG2ABcn2TDDOMuBN4OfHKu+5Ikzd0oZ/QbgUNVdbiqngN2AptnGPfbwG3AMyPsS5I0R6N8leAq4Ejf+lHgqv4BSf4nsKaq7knyf2d7oSRbga0Aa9euHaEkLTZ+Ld38ma+vpdPiM28fxiZ5EfCHwP851diq2l5Vk1U1uXLlyvkqSZKWpFGC/hiwpm99ddd20oXAS4D7kjwOvAKY8gNZSRqvUYJ+L7A+yboky4EtwNTJzqp6sqpWVNVEVU0ADwHXVtX0SBVLkk7LnIO+qk4ANwJ7gMeAD1TV/iS3JLn2TBUoSRrNKB/GUlW7gd0DbTfPMvanRtmXJGluvDNWkhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjRsp6JNsSnIwyaEk22bo/40kB5I8kuSfklw6yv4kSadvzkGfZBlwB/AaYANwfZINA8M+DUxW1Y8Au4Dfm+v+JElzM8oZ/UbgUFUdrqrngJ3A5v4BVfXPVfV0t/oQsHqE/UmS5mCUoF8FHOlbP9q1zeYG4MMj7E+SNAfnjGMnSX4ZmAR+cpb+rcBWgLVr146jJElaMkY5oz8GrOlbX921PU+Sa4B3AddW1bMzvVBVba+qyaqaXLly5QglSZIGjRL0e4H1SdYlWQ5sAab6ByR5GXAnvZD/jxH2JUmaozkHfVWdAG4E9gCPAR+oqv1JbklybTfs94ELgA8meTjJ1CwvJ0maJyPN0VfVbmD3QNvNfcvXjPL6kqTReWesJDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS48byDVPjNLHtnoUuoVmP3/rahS5B0hx4Ri9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjRgr6JJuSHExyKMm2GfrPS3J31//JJBOj7E+SdPrmHPRJlgF3AK8BNgDXJ9kwMOwG4KtV9UPA7cBtc92fJGluRjmj3wgcqqrDVfUcsBPYPDBmM3BXt7wL+JkkGWGfkqTTNMpXCa4CjvStHwWumm1MVZ1I8iTwvcCX+wcl2Qps7VafSnJwhLoWkxUM/C3OZvH/x2ARHTOP17cslWN26WwdZ8V3xlbVdmD7Qtcxbkmmq2pyoevQ8Dxmi4/HbLSpm2PAmr711V3bjGOSnAN8D/CVEfYpSTpNowT9XmB9knVJlgNbgKmBMVPAm7rl64B7q6pG2Kck6TTNeeqmm3O/EdgDLAN2VNX+JLcA01U1BfwZ8BdJDgFP0Hsz0LctuemqBnjMFp8lf8ziCbYktc07YyWpcQa9JDXOoD8LJXlPkncsdB3S2SzJRJLPztB+X5IlfTnlIIN+jNLj33yR6C4JlhY9Q2eedWcdB5P8OfBZ4N1J9iZ5JMlv9Y17V5LPJbkfuGzBCl5Ckry7Ozb3J3l/knd0Z4N/lGQaeHuSlyf5WJJ9SfYkuaTb9geT/GPX/i9JLu/a35fkT5I8mORwkusW9Jds3zlJ/irJY0l2Jfnu/s4kT/UtX5fkfd3yyiR/0/23uDfJ1WOue6w8YxmP9fTuJ7iI3v0EG4EAU0l+Avg6vUtPr6R3TD4F7FuYUpeGJD8KvAF4KXAuz/+bL6+qySTnAh8DNlfV8SRvBH4HeDO9S/beUlWfT3IV8F7gp7vtLwF+HLic3r0ku8b0ay1FlwE3VNUDSXYAvzbkdn8M3F5V9ydZS+8y8Svmq8iFZtCPxxer6qEkfwC8Gvh0134BvTeBC4G/raqnAZIM3nimM+9q4O+r6hngmSQf6uu7u/t5GfAS4KPds/iWAV9KcgHwY8AH+57Rd17f9n9XVd8EDiT5/nn8HQRHquqBbvkvgZuG3O4aYEPf8bsoyQVV9dQLbLNoGfTj8fXuZ4Dfrao7+zuT/Pr4S9IL6D9e+6vqlf2dSS4CvlZVV86y/bP9w+ehPn3b4I1AL7R+ft/yi4BXdG/0zXOOfrz2AG/uzghJsirJ9wEfB16f5LuSXAi8biGLXCIeAF6X5PzuePzcDGMOAiuTvBIgyblJXlxV/wl8IckvdO1J8tKxVa5+a08eH+AXgfsH+v89yRXdRRA/39f+EeBtJ1eSzPam3QSDfoyq6iPAXwOfSPIovbnbC6vqU/SmCz4DfJjec4Q0j6pqL73580fo/c0fBZ4cGPMcvc9UbkvyGeBhelM2AL8E3NC17+c7v4tB43EQeGuSx4CLgT8d6N8G/APwIPClvvabgMnuoogDwFvGUexC8REIWrJOzsl2V2p8HNjavelKTXGOXkvZ9u7rL88H7jLk1SrP6CWpcc7RS1LjzrqpmxUrVtTExMRClyFJi8q+ffu+XFUrZ+o764J+YmKC6enphS5DkhaVJF+crc+pG0lqnEEvSY0z6CWpcWfdHP2oJrbds9AlNOvxW1+70CVImgPP6CWpcUMFfZJN3Rc0HEqybYb+25M83P37XJKv9fV9o6/Px+9K0pidcuomyTLgDuBVwFFgb5KpqjpwckxV/e++8W8DXtb3Ev/1Ao9zlSTNs2HO6DcCh6rqcPc0v5288JP6rgfefyaKkySNbpigXwUc6Vs/2rV9hySXAuuAe/uaz08yneShJK+fc6WSpDk501fdbAF2VdU3+tourapjSX4AuDfJo1X1b/0bJdkKbAVYu3btGS5Jkpa2Yc7ojwFr+tZXd20z2cLAtE1VHet+Hgbu4/nz9yfHbK+qyaqaXLlyxkc1SJLmaJig3wusT7IuyXJ6Yf4dV88kuZzeN7x8oq/t4iTndcsr6H0h84HBbSVJ8+eUUzdVdSLJjfS+73QZsKOq9ie5BZiuqpOhvwXYWc9/wP0VwJ1JvknvTeXW/qt1JEnzb6g5+qraDeweaLt5YP09M2z3IPDDI9QnSRqRd8ZKUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNW6ooE+yKcnBJIeSbJuh/1eSHE/ycPfvV/v63pTk892/N53J4iVJp3bOqQYkWQbcAbwKOArsTTJVVQcGht5dVTcObPs/gN8EJoEC9nXbfvWMVC9JOqVhzug3Aoeq6nBVPQfsBDYP+fo/C3y0qp7owv2jwKa5lSpJmothgn4VcKRv/WjXNugNSR5JsivJmtPZNsnWJNNJpo8fPz5k6ZKkYZypD2M/BExU1Y/QO2u/63Q2rqrtVTVZVZMrV648QyVJkmC4oD8GrOlbX921fUtVfaWqnu1W/x/w8mG3lSTNr2GCfi+wPsm6JMuBLcBU/4Akl/StXgs81i3vAV6d5OIkFwOv7tokSWNyyqtuqupEkhvpBfQyYEdV7U9yCzBdVVPATUmuBU4ATwC/0m37RJLfpvdmAXBLVT0xD7+HJGkWpwx6gKraDeweaLu5b/mdwDtn2XYHsGOEGiVJI/DOWElqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNW6ooE+yKcnBJIeSbJuh/zeSHEjySJJ/SnJpX983kjzc/Zs6k8VLkk7tnFMNSLIMuAN4FXAU2JtkqqoO9A37NDBZVU8n+V/A7wFv7Pr+q6quPMN1S5KGNMwZ/UbgUFUdrqrngJ3A5v4BVfXPVfV0t/oQsPrMlilJmqthgn4VcKRv/WjXNpsbgA/3rZ+fZDrJQ0leP9MGSbZ2Y6aPHz8+REmSpGGdcurmdCT5ZWAS+Mm+5kur6liSHwDuTfJoVf1b/3ZVtR3YDjA5OVlnsiZJWuqGOaM/BqzpW1/dtT1PkmuAdwHXVtWzJ9ur6lj38zBwH/CyEeqVJJ2mYYJ+L7A+yboky4EtwPOunknyMuBOeiH/H33tFyc5r1teAVwN9H+IK0maZ6ecuqmqE0luBPYAy4AdVbU/yS3AdFVNAb8PXAB8MAnA/6+qa4ErgDuTfJPem8qtA1frSJLm2VBz9FW1G9g90HZz3/I1s2z3IPDDoxQoSRqNd8ZKUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxp3RRyBIp2ti2z0LXUKzHr/1tQtdgs4SntFLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mN81k3kk6LzyeaP/P1fCLP6CWpcQa9JDVuqKBPsinJwSSHkmybof+8JHd3/Z9MMtHX986u/WCSnz1zpUuShnHKoE+yDLgDeA2wAbg+yYaBYTcAX62qHwJuB27rtt0AbAFeDGwC3tu9niRpTIY5o98IHKqqw1X1HLAT2DwwZjNwV7e8C/iZJOnad1bVs1X1BeBQ93qSpDEZ5qqbVcCRvvWjwFWzjamqE0meBL63a39oYNtVgztIshXY2q0+leTgUNUvfiuALy90EcPKbQtdwVlh0Rwzj9e3LJVjdulsHWfF5ZVVtR3YvtB1jFuS6aqaXOg6NDyP2eLjMRtu6uYYsKZvfXXXNuOYJOcA3wN8ZchtJUnzaJig3wusT7IuyXJ6H65ODYyZAt7ULV8H3FtV1bVv6a7KWQesB/71zJQuSRrGKaduujn3G4E9wDJgR1XtT3ILMF1VU8CfAX+R5BDwBL03A7pxHwAOACeAt1bVN+bpd1mMltx0VQM8ZovPkj9m6Z14S5Ja5Z2xktQ4g16SGmfQn4WSvCfJOxa6DulslmQiyWdnaL8vyZK+nHKQQT9G6fFvvkh0lwpLi56hM8+6s46DSf4c+Czw7iR7kzyS5Lf6xr0ryeeS3A9ctmAFLyFJ3t0dm/uTvD/JO7qzwT9KMg28PcnLk3wsyb4ke5Jc0m37g0n+sWv/lySXd+3vS/InSR5McjjJdQv6S7bvnCR/leSxJLuSfHd/Z5Kn+pavS/K+bnllkr/p/lvcm+TqMdc9Vp6xjMd6evcZXETvPoONQICpJD8BfJ3eJalX0jsmnwL2LUypS0OSHwXeALwUOJfn/82XV9VkknOBjwGbq+p4kjcCvwO8md4le2+pqs8nuQp4L/DT3faXAD8OXE7vXpJdY/q1lqLLgBuq6oEkO4BfG3K7PwZur6r7k6yld/n4FfNV5EIz6Mfji1X1UJI/AF4NfLprv4Dem8CFwN9W1dMASQZvSNOZdzXw91X1DPBMkg/19d3d/bwMeAnw0d4z+lgGfCnJBcCPAR/s2gHO69v+76rqm8CBJN8/j7+D4EhVPdAt/yVw05DbXQNs6Dt+FyW5oKqeeoFtFi2Dfjy+3v0M8LtVdWd/Z5JfH39JegH9x2t/Vb2yvzPJRcDXqurKWbZ/tn/4PNSnbxu8EeiF1s/vW34R8Irujb55ztGP1x7gzd0ZIUlWJfk+4OPA65N8V5ILgdctZJFLxAPA65Kc3x2Pn5thzEFgZZJXAiQ5N8mLq+o/gS8k+YWuPUleOrbK1W/tyeMD/CJw/0D/vye5orsI4uf72j8CvO3kSpLZ3rSbYNCPUVV9BPhr4BNJHqU3d3thVX2K3nTBZ4AP03u+kOZRVe2lN3/+CL2/+aPAkwNjnqP3mcptST4DPExvygbgl4Abuvb9fOd3NGg8DgJvTfIYcDHwpwP924B/AB4EvtTXfhMw2V0UcQB4yziKXSg+AkFL1sk52e5KjY8DW7s3XakpztFrKdvefd3l+cBdhrxa5Rm9JDXOOXpJapxBL0mNM+glqXEGvSQ1zqCXpMb9N9Gfztcw9yd5AAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example creates a histogram for each probability distribution, allowing the probabilities for each event to be directly compared.\n",
        "\n",
        "We can see that indeed the distributions are different.\n",
        "\n",
        "Next, we can develop a function to calculate the KL divergence between the two distributions.\n",
        "\n",
        "We will use log base-2 to ensure the result has units in bits.\n",
        "\n",
        "```\n",
        "# calculate the kl divergence\n",
        "def kl_divergence(p, q):\n",
        "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
        "```\n",
        "\n",
        "We can then use this function to calculate the KL divergence of P from Q, as well as the reverse, Q from P.\n",
        "\n",
        "```\n",
        "# calculate (P || Q)\n",
        "kl_pq = kl_divergence(p, q)\n",
        "print('KL(P || Q): %.3f bits' % kl_pq)\n",
        "# calculate (Q || P)\n",
        "kl_qp = kl_divergence(q, p)\n",
        "print('KL(Q || P): %.3f bits' % kl_qp)\n",
        "```\n",
        "Tying this all together, the complete example is listed below."
      ],
      "metadata": {
        "id": "p-X0m6P999I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of calculating the kl divergence between two mass functions\n",
        "from math import log2\n",
        "\n",
        "# calculate the kl divergence\n",
        "def kl_divergence(p, q):\n",
        "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
        "\n",
        "# define distributions\n",
        "p = [0.10, 0.40, 0.50]\n",
        "q = [0.80, 0.15, 0.05]\n",
        "# calculate (P || Q)\n",
        "kl_pq = kl_divergence(p, q)\n",
        "print('KL(P || Q): %.3f bits' % kl_pq)\n",
        "# calculate (Q || P)\n",
        "kl_qp = kl_divergence(q, p)\n",
        "print('KL(Q || P): %.3f bits' % kl_qp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f68K0J09-eAu",
        "outputId": "994dfbf0-fd23-4c52-c0ea-21047d48eb9e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KL(P || Q): 1.927 bits\n",
            "KL(Q || P): 2.022 bits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example first calculates the divergence of P from Q as just under 2 bits, then Q from P as just over 2 bits.\n",
        "\n",
        "This is intuitive if we consider P has large probabilities when Q is small, giving P less divergence than Q from P as Q has more small probabilities when P has large probabilities. There is more divergence in this second case.\n",
        "\n",
        "If we change log2() to the natural logarithm log() function, the result is in nats, as follows:"
      ],
      "metadata": {
        "id": "NVneDBWN-uhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of calculating the kl divergence between two mass functions\n",
        "from math import log\n",
        "\n",
        "# calculate the kl divergence\n",
        "def kl_divergence(p, q):\n",
        "\treturn sum(p[i] * log(p[i]/q[i]) for i in range(len(p)))\n",
        "\n",
        "# define distributions\n",
        "p = [0.10, 0.40, 0.50]\n",
        "q = [0.80, 0.15, 0.05]\n",
        "# calculate (P || Q)\n",
        "kl_pq = kl_divergence(p, q)\n",
        "print('KL(P || Q): %.3f bits' % kl_pq)\n",
        "# calculate (Q || P)\n",
        "kl_qp = kl_divergence(q, p)\n",
        "print('KL(Q || P): %.3f bits' % kl_qp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cqKRALy-0Xt",
        "outputId": "e2293116-7329-4879-d91d-ca5008990570"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KL(P || Q): 1.336 bits\n",
            "KL(Q || P): 1.401 bits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SciPy library provides the kl_div() function for calculating the KL divergence, although with a different definition as defined here. It also provides the rel_entr() function for calculating the relative entropy, which matches the definition of KL divergence here. This is odd as “relative entropy” is often used as a synonym for “KL divergence.”\n",
        "\n",
        "Nevertheless, we can calculate the KL divergence using the rel_entr() SciPy function and confirm that our manual calculation is correct.\n",
        "\n",
        "The rel_entr() function takes lists of probabilities across all events from each probability distribution as arguments and returns a list of divergences for each event. These can be summed to give the KL divergence. The calculation uses the natural logarithm instead of log base-2 so the units are in nats instead of bits.\n",
        "\n",
        "The complete example using SciPy to calculate KL(P || Q) and KL(Q || P) for the same probability distributions used above is listed below:\n",
        "\n"
      ],
      "metadata": {
        "id": "r9GP_ozQ_WGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of calculating the kl divergence (relative entropy) with scipy\n",
        "from scipy.special import rel_entr\n",
        "# define distributions\n",
        "p = [0.10, 0.40, 0.50]\n",
        "q = [0.80, 0.15, 0.05]\n",
        "# calculate (P || Q)\n",
        "kl_pq = rel_entr(p, q)\n",
        "print('KL(P || Q): %.3f nats' % sum(kl_pq))\n",
        "# calculate (Q || P)\n",
        "kl_qp = rel_entr(q, p)\n",
        "print('KL(Q || P): %.3f nats' % sum(kl_qp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0-yTDlW_WxB",
        "outputId": "5b696e6c-f9dc-4772-805d-dcabb6c149a6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KL(P || Q): 1.336 nats\n",
            "KL(Q || P): 1.401 nats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example, we can see that the calculated divergences match our manual calculation of about 1.3 nats and about 1.4 nats for KL(P || Q) and KL(Q || P) respectively."
      ],
      "metadata": {
        "id": "p8JmLYtO_cwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Jensen-Shannon Divergence"
      ],
      "metadata": {
        "id": "AN5rKph8_exY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [Jensen-Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence), or JS divergence for short, is another way to quantify the difference (or similarity) between two probability distributions.\n",
        "\n",
        "It uses the KL divergence to calculate a normalized score that is symmetrical. This means that the divergence of P from Q is the same as Q from P, or stated formally:\n",
        "\n",
        "```JS(P || Q) == JS(Q || P)```\n",
        "\n",
        "The JS divergence can be calculated as follows:\n",
        "\n",
        "```JS(P || Q) = 1/2 * KL(P || M) + 1/2 * KL(Q || M)```\n",
        "\n",
        "Where M is calculated as:\n",
        "\n",
        "```M = 1/2 * (P + Q)```\n",
        "\n",
        "And KL() is calculated as the KL divergence described in the previous section.\n",
        "\n",
        "It is more useful as a measure as it provides a smoothed and normalized version of KL divergence, with scores between 0 (identical) and 1 (maximally different), when using the base-2 logarithm.\n",
        "\n",
        "The square root of the score gives a quantity referred to as the Jensen-Shannon distance, or JS distance for short.\n",
        "\n",
        "We can make the JS divergence concrete with a worked example.\n",
        "\n",
        "First, we can define a function to calculate the JS divergence that uses the kl_divergence() function prepared in the previous section.\n",
        "\n",
        "```\n",
        "# calculate the kl divergence\n",
        "def kl_divergence(p, q):\n",
        "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
        "\n",
        "# calculate the js divergence\n",
        "def js_divergence(p, q):\n",
        "\tm = 0.5 * (p + q)\n",
        "\treturn 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n",
        "```\n",
        "\n",
        "We can then test this function using the same probability distributions used in the previous section.\n",
        "\n",
        "First, we will calculate the JS divergence score for the distributions, then calculate the square root of the score to give the JS distance between the distributions. For example:\n",
        "\n",
        "```\n",
        "...\n",
        "# calculate JS(P || Q)\n",
        "js_pq = js_divergence(p, q)\n",
        "print('JS(P || Q) divergence: %.3f bits' % js_pq)\n",
        "print('JS(P || Q) distance: %.3f' % sqrt(js_pq))\n",
        "```\n",
        "\n",
        "This can then be repeated for the reverse case to show that the divergence is symmetrical, unlike the KL divergence.\n",
        "\n",
        "```\n",
        "...\n",
        "# calculate JS(Q || P)\n",
        "js_qp = js_divergence(q, p)\n",
        "print('JS(Q || P) divergence: %.3f bits' % js_qp)\n",
        "print('JS(Q || P) distance: %.3f' % sqrt(js_qp))\n",
        "```\n",
        "Tying this together, the complete example of calculating the JS divergence and JS distance is listed below."
      ],
      "metadata": {
        "id": "CYVh7OHR_gEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of calculating the js divergence between two mass functions\n",
        "from math import log2\n",
        "from math import sqrt\n",
        "from numpy import asarray\n",
        "\n",
        "# calculate the kl divergence\n",
        "def kl_divergence(p, q):\n",
        "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
        "\n",
        "# calculate the js divergence\n",
        "def js_divergence(p, q):\n",
        "\tm = 0.5 * (p + q)\n",
        "\treturn 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n",
        "\n",
        "# define distributions\n",
        "p = asarray([0.10, 0.40, 0.50])\n",
        "q = asarray([0.80, 0.15, 0.05])\n",
        "# calculate JS(P || Q)\n",
        "js_pq = js_divergence(p, q)\n",
        "print('JS(P || Q) divergence: %.3f bits' % js_pq)\n",
        "print('JS(P || Q) distance: %.3f' % sqrt(js_pq))\n",
        "# calculate JS(Q || P)\n",
        "js_qp = js_divergence(q, p)\n",
        "print('JS(Q || P) divergence: %.3f bits' % js_qp)\n",
        "print('JS(Q || P) distance: %.3f' % sqrt(js_qp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFmNihMiBCaR",
        "outputId": "db65cec3-c0d3-4052-e2fa-aa31dc286d5d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JS(P || Q) divergence: 0.420 bits\n",
            "JS(P || Q) distance: 0.648\n",
            "JS(Q || P) divergence: 0.420 bits\n",
            "JS(Q || P) distance: 0.648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example shows that the JS divergence between the distributions is about 0.4 bits and that the distance is about 0.6.\n",
        "\n",
        "We can see that the calculation is symmetrical, giving the same score and distance measure for JS(P || Q) and JS(Q || P).\n",
        "\n"
      ],
      "metadata": {
        "id": "qTIkxn-OBL27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3X0FuySpBPPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SciPy library provides an implementation of the JS distance via the jensenshannon() function.\n",
        "\n",
        "It takes arrays of probabilities across all events from each probability distribution as arguments and returns the JS distance score, not a divergence score. We can use this function to confirm our manual calculation of the JS distance.\n",
        "\n",
        "The complete example is listed below."
      ],
      "metadata": {
        "id": "wSsRJCULBPR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the jensen-shannon distance metric\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from numpy import asarray\n",
        "# define distributions\n",
        "p = asarray([0.10, 0.40, 0.50])\n",
        "q = asarray([0.80, 0.15, 0.05])\n",
        "# calculate JS(P || Q)\n",
        "js_pq = jensenshannon(p, q, base=2)\n",
        "print('JS(P || Q) Distance: %.3f' % js_pq)\n",
        "# calculate JS(Q || P)\n",
        "js_qp = jensenshannon(q, p, base=2)\n",
        "print('JS(Q || P) Distance: %.3f' % js_qp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0yYfkAdBP-c",
        "outputId": "fe234560-35c7-4116-8945-6c7cd916a92f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JS(P || Q) Distance: 0.648\n",
            "JS(Q || P) Distance: 0.648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example, we can confirm the distance score matches our manual calculation of 0.648, and that the distance calculation is symmetrical as expected."
      ],
      "metadata": {
        "id": "zH-AM0kyBXHt"
      }
    }
  ]
}