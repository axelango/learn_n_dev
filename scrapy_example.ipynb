{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scrapy_example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMwFqktWDvqnLpqiOMdy8JY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgamel/learn_n_dev/blob/python_web_scrapping/scrapy_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scrapping Example using Scrapy Module "
      ],
      "metadata": {
        "id": "RY8kVybTM61j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Drive:"
      ],
      "metadata": {
        "id": "EJy58c03NJBI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n8xDHzJGnLN",
        "outputId": "06eec661-e375-43cf-da1b-20d5681bc5da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Colab Notebooks')"
      ],
      "metadata": {
        "id": "u3GzE7IdG0Ja"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Scrapy code from Jupyter Notebook without issues"
      ],
      "metadata": {
        "id": "Y4lkJVUINMlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrapy is an open-source framework for extracting the data from websites. It is fast, simple, and extensible. Every data scientist should have familiarity with this, as they often need to gather data in this manner. Data scientists usually prefer some sort of computational notebook for managing their workflow. Jupyter Notebook is very popular amid data scientists among other options like PyCharm, zeppelin, VS Code, nteract, Google Colab, and spyder to name a few."
      ],
      "metadata": {
        "id": "UwgVrDlvNRUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scraping using Scrapy is done with a .py file often. It can be also initialized from a Notebook. The problem with that is, it throws an error `ReactorNotRestartable:` when the code block is run for the second time."
      ],
      "metadata": {
        "id": "2aGcJprCNYvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a work-around for this error using crochet package. ReactorNotRestartable error can be mitigated using this package. In this blog post, I am showing the steps that I took to run scrapy codes from Jupyter Notebook with out the error."
      ],
      "metadata": {
        "id": "bFu-ENa0NdCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demo Project:"
      ],
      "metadata": {
        "id": "go5p1AWeNoC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "from scrapy.crawler import CrawlerRunner\n",
        "# text cleaning\n",
        "import re\n",
        "# Reactor restart\n",
        "from crochet import setup, wait_for\n",
        "setup()"
      ],
      "metadata": {
        "id": "lRGYA06AGrWx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuotesToCsv(scrapy.Spider):\n",
        "    \"\"\"scrape first line of  quotes from `wikiquote` by \n",
        "    Maynard James Keenan and save to json file\"\"\"\n",
        "    name = \"MJKQuotesToCsv\"\n",
        "    start_urls = [\n",
        "        'https://en.wikiquote.org/wiki/Mahatma_Gandhi',\n",
        "    ]\n",
        "    custom_settings = {\n",
        "        'ITEM_PIPELINES': {\n",
        "            '__main__.ExtractFirstLine': 1\n",
        "        },\n",
        "        'FEEDS': {\n",
        "            '/content/gdrive/My Drive/quotes.csv': {\n",
        "                'format': 'csv',\n",
        "                'overwrite': True\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    def parse(self, response):\n",
        "        \"\"\"parse data from urls\"\"\"\n",
        "        for quote in response.css('div.mw-parser-output > ul > li'):\n",
        "            yield {'quote': quote.extract()}\n",
        "\n",
        "\n",
        "class ExtractFirstLine(object):\n",
        "    def process_item(self, item, spider):\n",
        "        \"\"\"text processing\"\"\"\n",
        "        lines = dict(item)[\"quote\"].splitlines()\n",
        "        first_line = self.__remove_html_tags__(lines[0])\n",
        "\n",
        "        return {'quote': first_line}\n",
        "\n",
        "    def __remove_html_tags__(self, text):\n",
        "        \"\"\"remove html tags from string\"\"\"\n",
        "        html_tags = re.compile('<.*?>')\n",
        "        return re.sub(html_tags, '', text)\n",
        "\n",
        "@wait_for(10)\n",
        "def run_spider():\n",
        "    \"\"\"run spider with MJKQuotesToCsv\"\"\"\n",
        "    crawler = CrawlerRunner()\n",
        "    d = crawler.crawl(QuotesToCsv)\n",
        "    return d\n",
        "\n",
        "run_spider()"
      ],
      "metadata": {
        "id": "VHZ8wycTHkQn"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Using CrawlerRunner instead of CrawlerProcess .\n",
        "\n",
        "2. Importing setup and wait_for from crochet and initializing using setup() .\n",
        "\n",
        "3. Using @wait_for(10) decorator on the function that runs the spider from scrapy. @wait_for is used for blocking calls into Twisted Reactor thread. Click here to learn more about this."
      ],
      "metadata": {
        "id": "g6MNpQvMNz6x"
      }
    }
  ]
}