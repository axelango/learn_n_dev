{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_Regression_From_Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMrMHrt6a2YO6fLyykqrt/f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgamel/learn_n_dev/blob/python_modeling_forecasting/Logistic_Regression_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement Logistic Regression From Scratch in Python"
      ],
      "metadata": {
        "id": "E7f2afMVZcQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is the go-to linear classification algorithm for two-class problems.\n",
        "\n",
        "It is easy to implement, easy to understand and gets great results on a wide variety of problems, even when the expectations the method has of your data are violated.\n",
        "\n",
        "In this tutorial, you will discover how to implement logistic regression with stochastic gradient descent from scratch with Python."
      ],
      "metadata": {
        "id": "j7rrJqLAZi4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this script:\n",
        "\n",
        "* How to make predictions with a logistic regression model.\n",
        "* How to estimate coefficients using stochastic gradient descent.\n",
        "* How to apply logistic regression to a real prediction problem."
      ],
      "metadata": {
        "id": "feuy8KO5ZlX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Description\n",
        "\n",
        "This section will give a brief description of the logistic regression technique, stochastic gradient descent and the Pima Indians diabetes dataset we will use in this tutorial.\n",
        "\n"
      ],
      "metadata": {
        "id": "XQH4ato1Zy9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression\n",
        "\n",
        "Logistic regression is named for the function used at the core of the method, the logistic function.\n",
        "\n",
        "Logistic regression uses an equation as the representation, very much like linear regression. Input values (X) are combined linearly using weights or coefficient values to predict an output value (y).\n",
        "\n",
        "A key difference from linear regression is that the output value being modeled is a binary value (0 or 1) rather than a numeric value.\n",
        "\n",
        "```yhat = e^(b0 + b1 * x1) / (1 + e^(b0 + b1 * x1))```\n",
        "\n",
        "This can be simplified as:\n",
        "\n",
        "```yhat = 1.0 / (1.0 + e^(-(b0 + b1 * x1)))```\n",
        "\n",
        "Where e is the base of the natural logarithms (Euler’s number), yhat is the predicted output, b0 is the bias or intercept term and b1 is the coefficient for the single input value (x1).\n",
        "\n",
        "The yhat prediction is a real value between 0 and 1, that needs to be rounded to an integer value and mapped to a predicted class value.\n",
        "\n",
        "Each column in your input data has an associated b coefficient (a constant real value) that must be learned from your training data. The actual representation of the model that you would store in memory or in a file are the coefficients in the equation (the beta value or b’s).\n",
        "\n",
        "The coefficients of the logistic regression algorithm must be estimated from your training data."
      ],
      "metadata": {
        "id": "NP6wl0-zZ1xl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "2GACGq2kaHBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent is the process of minimizing a function by following the gradients of the cost function.\n",
        "\n",
        "This involves knowing the form of the cost as well as the derivative so that from a given point you know the gradient and can move in that direction, e.g. downhill towards the minimum value.\n",
        "\n",
        "In machine learning, we can use a technique that evaluates and updates the coefficients every iteration called stochastic gradient descent to minimize the error of a model on our training data.\n",
        "\n",
        "The way this optimization algorithm works is that each training instance is shown to the model one at a time. The model makes a prediction for a training instance, the error is calculated and the model is updated in order to reduce the error for the next prediction.\n",
        "\n",
        "This procedure can be used to find the set of coefficients in a model that result in the smallest error for the model on the training data. Each iteration, the coefficients (b) in machine learning language are updated using the equation:\n",
        "\n",
        "```b = b + learning_rate * (y - yhat) * yhat * (1 - yhat) * x```\n",
        "\n",
        "Where b is the coefficient or weight being optimized, learning_rate is a learning rate that you must configure (e.g. 0.01), (y – yhat) is the prediction error for the model on the training data attributed to the weight, yhat is the prediction made by the coefficients and x is the input value.\n"
      ],
      "metadata": {
        "id": "uE_SVN9_aoPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pima Indians Diabetes Dataset"
      ],
      "metadata": {
        "id": "cwQANaqEbos4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pima Indians dataset involves predicting the onset of diabetes within 5 years in Pima Indians given basic medical details.\n",
        "\n",
        "* Dataset File.\n",
        "* Dataset Details.\n",
        "\n",
        "It is a binary classification problem, where the prediction is either 0 (no diabetes) or 1 (diabetes).\n",
        "\n",
        "It contains 768 rows and 9 columns. All of the values in the file are numeric, specifically floating point values. Below is a small sample of the first few rows of the problem.\n",
        "\n",
        "```\n",
        "6,148,72,35,0,33.6,0.627,50,1\n",
        "1,85,66,29,0,26.6,0.351,31,0\n",
        "8,183,64,0,0,23.3,0.672,32,1\n",
        "1,89,66,23,94,28.1,0.167,21,0\n",
        "0,137,40,35,168,43.1,2.288,33,1\n",
        "...\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "rLIv9EN6bqHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making Predictions"
      ],
      "metadata": {
        "id": "0M4BUSrGevOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to develop a function that can make predictions.\n",
        "\n",
        "This will be needed both in the evaluation of candidate coefficient values in stochastic gradient descent and after the model is finalized and we wish to start making predictions on test data or new data.\n",
        "\n",
        "Below is a function named predict() that predicts an output value for a row given a set of coefficients.\n",
        "\n",
        "The first coefficient in is always the intercept, also called the bias or b0 as it is standalone and not responsible for a specific input value.\n",
        "\n",
        "```\n",
        "# Make a prediction with coefficients\n",
        "def predict(row, coefficients):\n",
        "\tyhat = coefficients[0]\n",
        "\tfor i in range(len(row)-1):\n",
        "\t\tyhat += coefficients[i + 1] * row[i]\n",
        "\treturn 1.0 / (1.0 + exp(-yhat))\n",
        "```\n",
        "\n",
        "We can contrive a small dataset to test our predict() function.\n",
        "\n",
        "```\n",
        "X1\t\tX2\t\tY\n",
        "2.7810836\t2.550537003\t0\n",
        "1.465489372\t2.362125076\t0\n",
        "3.396561688\t4.400293529\t0\n",
        "1.38807019\t1.850220317\t0\n",
        "3.06407232\t3.005305973\t0\n",
        "7.627531214\t2.759262235\t1\n",
        "5.332441248\t2.088626775\t1\n",
        "6.922596716\t1.77106367\t1\n",
        "8.675418651\t-0.242068655\t1\n",
        "7.673756466\t3.508563011\t1\n",
        "```\n",
        "\n",
        "Below is a plot of the dataset using different colors to show the different classes for each point.\n",
        "\n",
        "![Small-Contrived-Classification-Dataset.webp](data:image/webp;base64,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)\n",
        "\n",
        "We can also use previously prepared coefficients to make predictions for this dataset.\n",
        "\n",
        "Putting this all together we can test our predict() function below."
      ],
      "metadata": {
        "id": "vlX9h38hewqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction\n",
        "from math import exp\n",
        "\n",
        "# Make a prediction with coefficients\n",
        "def predict(row, coefficients):\n",
        "\tyhat = coefficients[0]\n",
        "\tfor i in range(len(row)-1):\n",
        "\t\tyhat += coefficients[i + 1] * row[i]\n",
        "\treturn 1.0 / (1.0 + exp(-yhat))\n",
        "\n",
        "# test predictions\n",
        "dataset = [[2.7810836,2.550537003,0],\n",
        "\t[1.465489372,2.362125076,0],\n",
        "\t[3.396561688,4.400293529,0],\n",
        "\t[1.38807019,1.850220317,0],\n",
        "\t[3.06407232,3.005305973,0],\n",
        "\t[7.627531214,2.759262235,1],\n",
        "\t[5.332441248,2.088626775,1],\n",
        "\t[6.922596716,1.77106367,1],\n",
        "\t[8.675418651,-0.242068655,1],\n",
        "\t[7.673756466,3.508563011,1]]\n",
        "coef = [-0.406605464, 0.852573316, -1.104746259]\n",
        "for row in dataset:\n",
        "\tyhat = predict(row, coef)\n",
        "\tprint(\"Expected=%.3f, Predicted=%.3f [%d]\" % (row[-1], yhat, round(yhat)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-nqMR1Zl1qB",
        "outputId": "88c89c5b-c6a2-481f-e213-2db8d088ab76"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected=0.000, Predicted=0.299 [0]\n",
            "Expected=0.000, Predicted=0.146 [0]\n",
            "Expected=0.000, Predicted=0.085 [0]\n",
            "Expected=0.000, Predicted=0.220 [0]\n",
            "Expected=0.000, Predicted=0.247 [0]\n",
            "Expected=1.000, Predicted=0.955 [1]\n",
            "Expected=1.000, Predicted=0.862 [1]\n",
            "Expected=1.000, Predicted=0.972 [1]\n",
            "Expected=1.000, Predicted=0.999 [1]\n",
            "Expected=1.000, Predicted=0.905 [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two inputs values (X1 and X2) and three coefficient values (b0, b1 and b2). The prediction equation we have modeled for this problem is:\n",
        "\n",
        "```y = 1.0 / (1.0 + e^(-(b0 + b1 * X1 + b2 * X2)))```\n",
        "\n",
        "or, with the specific coefficient values we chose by hand as:\n",
        "\n",
        "```y = 1.0 / (1.0 + e^(-(-0.406605464 + 0.852573316 * X1 + -1.104746259 * X2)))```\n",
        "\n",
        "Running this function we get predictions that are reasonably close to the expected output (y) values and when rounded make correct predictions of the class.\n",
        "\n",
        "```\n",
        "Expected=0.000, Predicted=0.299 [0]\n",
        "Expected=0.000, Predicted=0.146 [0]\n",
        "Expected=0.000, Predicted=0.085 [0]\n",
        "Expected=0.000, Predicted=0.220 [0]\n",
        "Expected=0.000, Predicted=0.247 [0]\n",
        "Expected=1.000, Predicted=0.955 [1]\n",
        "Expected=1.000, Predicted=0.862 [1]\n",
        "Expected=1.000, Predicted=0.972 [1]\n",
        "Expected=1.000, Predicted=0.999 [1]\n",
        "Expected=1.000, Predicted=0.905 [1]\n",
        "```\n",
        "\n",
        "Now we are ready to implement stochastic gradient descent to optimize our coefficient values.\n"
      ],
      "metadata": {
        "id": "m16D7XKgmOl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estimating Coefficients"
      ],
      "metadata": {
        "id": "bp_z2HRHoGsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can estimate the coefficient values for our training data using stochastic gradient descent.\n",
        "\n",
        "Stochastic gradient descent requires two parameters:\n",
        "\n",
        "* Learning Rate: Used to limit the amount each coefficient is corrected each time it is updated.\n",
        "* Epochs: The number of times to run through the training data while updating the coefficients.\n",
        "\n",
        "These, along with the training data will be the arguments to the function.\n",
        "\n",
        "There are 3 loops we need to perform in the function:\n",
        "\n",
        "1. Loop over each epoch.\n",
        "2. Loop over each row in the training data for an epoch.\n",
        "3. Loop over each coefficient and update it for a row in an epoch.\n",
        "\n",
        "As you can see, we update each coefficient for each row in the training data, each epoch.\n",
        "\n",
        "Coefficients are updated based on the error the model made. The error is calculated as the difference between the expected output value and the prediction made with the candidate coefficients.\n",
        "\n",
        "There is one coefficient to weight each input attribute, and these are updated in a consistent way, for example:\n",
        "\n",
        "```b1(t+1) = b1(t) + learning_rate * (y(t) - yhat(t)) * yhat(t) * (1 - yhat(t)) * x1(t)```\n",
        "\n",
        "The special coefficient at the beginning of the list, also called the intercept, is updated in a similar way, except without an input as it is not associated with a specific input value:\n",
        "\n",
        "```b0(t+1) = b0(t) + learning_rate * (y(t) - yhat(t)) * yhat(t) * (1 - yhat(t))```\n",
        "\n",
        "Now we can put all of this together. Below is a function named coefficients_sgd() that calculates coefficient values for a training dataset using stochastic gradient descent.\n",
        "\n",
        "```\n",
        "# Estimate logistic regression coefficients using stochastic gradient descent\n",
        "def coefficients_sgd(train, l_rate, n_epoch):\n",
        "\tcoef = [0.0 for i in range(len(train[0]))]\n",
        "\tfor epoch in range(n_epoch):\n",
        "\t\tsum_error = 0\n",
        "\t\tfor row in train:\n",
        "\t\t\tyhat = predict(row, coef)\n",
        "\t\t\terror = row[-1] - yhat\n",
        "\t\t\tsum_error += error**2\n",
        "\t\t\tcoef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
        "\t\t\tfor i in range(len(row)-1):\n",
        "\t\t\t\tcoef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
        "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
        "\treturn coef\n",
        "```\n",
        "\n",
        "You can see, that in addition, we keep track of the sum of the squared error (a positive value) each epoch so that we can print out a nice message each outer loop.\n",
        "\n",
        "We can test this function on the same small contrived dataset from above.\n",
        "\n",
        "```\n",
        "from math import exp\n",
        "\n",
        "# Make a prediction with coefficients\n",
        "def predict(row, coefficients):\n",
        "\tyhat = coefficients[0]\n",
        "\tfor i in range(len(row)-1):\n",
        "\t\tyhat += coefficients[i + 1] * row[i]\n",
        "\treturn 1.0 / (1.0 + exp(-yhat))\n",
        "\n",
        "# Estimate logistic regression coefficients using stochastic gradient descent\n",
        "def coefficients_sgd(train, l_rate, n_epoch):\n",
        "\tcoef = [0.0 for i in range(len(train[0]))]\n",
        "\tfor epoch in range(n_epoch):\n",
        "\t\tsum_error = 0\n",
        "\t\tfor row in train:\n",
        "\t\t\tyhat = predict(row, coef)\n",
        "\t\t\terror = row[-1] - yhat\n",
        "\t\t\tsum_error += error**2\n",
        "\t\t\tcoef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
        "\t\t\tfor i in range(len(row)-1):\n",
        "\t\t\t\tcoef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
        "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
        "\treturn coef\n",
        "\n",
        "# Calculate coefficients\n",
        "dataset = [[2.7810836,2.550537003,0],\n",
        "\t[1.465489372,2.362125076,0],\n",
        "\t[3.396561688,4.400293529,0],\n",
        "\t[1.38807019,1.850220317,0],\n",
        "\t[3.06407232,3.005305973,0],\n",
        "\t[7.627531214,2.759262235,1],\n",
        "\t[5.332441248,2.088626775,1],\n",
        "\t[6.922596716,1.77106367,1],\n",
        "\t[8.675418651,-0.242068655,1],\n",
        "\t[7.673756466,3.508563011,1]]\n",
        "l_rate = 0.3\n",
        "n_epoch = 100\n",
        "coef = coefficients_sgd(dataset, l_rate, n_epoch)\n",
        "print(coef)\n",
        "```\n",
        "\n",
        "We use a larger learning rate of 0.3 and train the model for 100 epochs, or 100 exposures of the coefficients to the entire training dataset.\n",
        "\n",
        "Running the example prints a message each epoch with the sum squared error for that epoch and the final set of coefficients.\n",
        "\n",
        "```\n",
        ">epoch=95, lrate=0.300, error=0.023\n",
        ">epoch=96, lrate=0.300, error=0.023\n",
        ">epoch=97, lrate=0.300, error=0.023\n",
        ">epoch=98, lrate=0.300, error=0.023\n",
        ">epoch=99, lrate=0.300, error=0.022\n",
        "[-0.8596443546618897, 1.5223825112460005, -2.218700210565016]\n",
        "```\n",
        "\n",
        "You can see how error continues to drop even in the final epoch. We could probably train for a lot longer (more epochs) or increase the amount we update the coefficients each epoch (higher learning rate).\n",
        "\n",
        "Experiment and see what you come up with.\n",
        "\n",
        "Now, let’s apply this algorithm on a real dataset."
      ],
      "metadata": {
        "id": "485KmaCvoIGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diabetes Prediction"
      ],
      "metadata": {
        "id": "ksV_3QBWprKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will train a logistic regression model using stochastic gradient descent on the diabetes dataset.\n",
        "\n",
        "The example assumes that a CSV copy of the dataset is in the current working directory with the filename pima-indians-diabetes.csv.\n",
        "\n",
        "The dataset is first loaded, the string values converted to numeric and each column is normalized to values in the range of 0 to 1. This is achieved with the helper functions load_csv() and str_column_to_float() to load and prepare the dataset and dataset_minmax() and normalize_dataset() to normalize it.\n",
        "\n",
        "We will use k-fold cross validation to estimate the performance of the learned model on unseen data. This means that we will construct and evaluate k models and estimate the performance as the mean model performance. Classification accuracy will be used to evaluate each model. These behaviors are provided in the cross_validation_split(), accuracy_metric() and evaluate_algorithm() helper functions.\n",
        "\n",
        "We will use the predict(), coefficients_sgd() functions created above and a new logistic_regression() function to train the model.\n",
        "\n",
        "Below is the complete example."
      ],
      "metadata": {
        "id": "-IIdCWq4ptlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwXzxk0uqt5-",
        "outputId": "f2f18ef7-2b19-4fde-afb9-e018ef7f2821"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression on Diabetes Dataset\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from csv import reader\n",
        "from math import exp\n",
        "\n",
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "\tdataset = list()\n",
        "\twith open(filename, 'r') as file:\n",
        "\t\tcsv_reader = reader(file)\n",
        "\t\tfor row in csv_reader:\n",
        "\t\t\tif not row:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tdataset.append(row)\n",
        "\treturn dataset\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = float(row[column].strip())\n",
        "\n",
        "# Find the min and max values for each column\n",
        "def dataset_minmax(dataset):\n",
        "\tminmax = list()\n",
        "\tfor i in range(len(dataset[0])):\n",
        "\t\tcol_values = [row[i] for row in dataset]\n",
        "\t\tvalue_min = min(col_values)\n",
        "\t\tvalue_max = max(col_values)\n",
        "\t\tminmax.append([value_min, value_max])\n",
        "\treturn minmax\n",
        "\n",
        "# Rescale dataset columns to the range 0-1\n",
        "def normalize_dataset(dataset, minmax):\n",
        "\tfor row in dataset:\n",
        "\t\tfor i in range(len(row)):\n",
        "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
        "\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "\tdataset_split = list()\n",
        "\tdataset_copy = list(dataset)\n",
        "\tfold_size = int(len(dataset) / n_folds)\n",
        "\tfor i in range(n_folds):\n",
        "\t\tfold = list()\n",
        "\t\twhile len(fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\tdataset_split.append(fold)\n",
        "\treturn dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\treturn correct / float(len(actual)) * 100.0\n",
        "\n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "\tfolds = cross_validation_split(dataset, n_folds)\n",
        "\tscores = list()\n",
        "\tfor fold in folds:\n",
        "\t\ttrain_set = list(folds)\n",
        "\t\ttrain_set.remove(fold)\n",
        "\t\ttrain_set = sum(train_set, [])\n",
        "\t\ttest_set = list()\n",
        "\t\tfor row in fold:\n",
        "\t\t\trow_copy = list(row)\n",
        "\t\t\ttest_set.append(row_copy)\n",
        "\t\t\trow_copy[-1] = None\n",
        "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
        "\t\tactual = [row[-1] for row in fold]\n",
        "\t\taccuracy = accuracy_metric(actual, predicted)\n",
        "\t\tscores.append(accuracy)\n",
        "\treturn scores\n",
        "\n",
        "# Make a prediction with coefficients\n",
        "def predict(row, coefficients):\n",
        "\tyhat = coefficients[0]\n",
        "\tfor i in range(len(row)-1):\n",
        "\t\tyhat += coefficients[i + 1] * row[i]\n",
        "\treturn 1.0 / (1.0 + exp(-yhat))\n",
        "\n",
        "# Estimate logistic regression coefficients using stochastic gradient descent\n",
        "def coefficients_sgd(train, l_rate, n_epoch):\n",
        "\tcoef = [0.0 for i in range(len(train[0]))]\n",
        "\tfor epoch in range(n_epoch):\n",
        "\t\tfor row in train:\n",
        "\t\t\tyhat = predict(row, coef)\n",
        "\t\t\terror = row[-1] - yhat\n",
        "\t\t\tcoef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
        "\t\t\tfor i in range(len(row)-1):\n",
        "\t\t\t\tcoef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
        "\treturn coef\n",
        "\n",
        "# Linear Regression Algorithm With Stochastic Gradient Descent\n",
        "def logistic_regression(train, test, l_rate, n_epoch):\n",
        "\tpredictions = list()\n",
        "\tcoef = coefficients_sgd(train, l_rate, n_epoch)\n",
        "\tfor row in test:\n",
        "\t\tyhat = predict(row, coef)\n",
        "\t\tyhat = round(yhat)\n",
        "\t\tpredictions.append(yhat)\n",
        "\treturn(predictions)\n",
        "\n",
        "# Test the logistic regression algorithm on the diabetes dataset\n",
        "seed(1)\n",
        "# load and prepare data\n",
        "filename = '/content/gdrive/My Drive/input_examples/pima-indians-diabetes.csv'\n",
        "dataset = load_csv(filename)\n",
        "for i in range(len(dataset[0])):\n",
        "\tstr_column_to_float(dataset, i)\n",
        "# normalize\n",
        "minmax = dataset_minmax(dataset)\n",
        "normalize_dataset(dataset, minmax)\n",
        "# evaluate algorithm\n",
        "n_folds = 5\n",
        "l_rate = 0.1\n",
        "n_epoch = 100\n",
        "scores = evaluate_algorithm(dataset, logistic_regression, n_folds, l_rate, n_epoch)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuaBn4R3sYOI",
        "outputId": "03376da8-9261-45f9-e8c3-cd4276167a93"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [73.8562091503268, 78.43137254901961, 81.69934640522875, 75.81699346405229, 75.81699346405229]\n",
            "Mean Accuracy: 77.124%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A k value of 5 was used for cross-validation, giving each fold 768/5 = 153.6 or just over 150 records to be evaluated upon each iteration. A learning rate of 0.1 and 100 training epochs were chosen with a little experimentation.\n",
        "\n",
        "You can try your own configurations and see if you can beat my score.\n",
        "\n",
        "Running this example prints the scores for each of the 5 cross-validation folds, then prints the mean classification accuracy.\n",
        "\n",
        "We can see that the accuracy is about 77%, higher than the baseline value of 65% if we just predicted the majority class using the Zero Rule Algorithm."
      ],
      "metadata": {
        "id": "L2nwyLEku2SB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extensions"
      ],
      "metadata": {
        "id": "p3-o8KR7_p7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section lists a number of extensions to this tutorial that you may wish to consider exploring.\n",
        "\n",
        "* Tune The Example. Tune the learning rate, number of epochs and even data preparation method to get an improved score on the dataset.\n",
        "* Batch Stochastic Gradient Descent. Change the stochastic gradient descent algorithm to accumulate updates across each epoch and only update the coefficients in a batch at the end of the epoch.\n",
        "* Additional Classification Problems. Apply the technique to other binary (2 class) classification problems on the UCI machine learning repository."
      ],
      "metadata": {
        "id": "YAmQGZs__tYC"
      }
    }
  ]
}